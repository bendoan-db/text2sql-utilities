{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1ebe40-c652-4d9c-951c-1faa290d1d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./ingestion_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "databricks_config = config[\"databricks_config\"]\n",
    "volume_path = databricks_config[\"volume_path\"]\n",
    "\n",
    "catalog = config[\"databricks_config\"][\"catalog\"]\n",
    "metadata_path = f\"{volume_path}dev_20240627/dev_tables.json\"\n",
    "sql_lite_fqdn = f\"{volume_path}dev_20240627/dev_databases/dev_databases/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f672320b-3e5e-4c9b-b91a-6884226e2ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(display(spark.read.option(\"multiline\", True).json(metadata_path)))\n",
    "table_metadata = spark.read.option(\"multiline\", True).json(metadata_path).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49781864-5459-448d-a20e-8d6f658fbdd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "def clean_column_names(df):\n",
    "    new_column_names = [re.sub(r'[ ,;{}()\\n\\t=]', '_', col) for col in df.columns]\n",
    "    return df.toDF(*new_column_names)\n",
    "\n",
    "def table_exists(catalog, schema, table):\n",
    "  try:\n",
    "    query = f\"SHOW TABLES IN {catalog}.{schema} LIKE '{table}'\"\n",
    "    result = spark.sql(query).collect()\n",
    "    return len(result) > 0\n",
    "  except Exception as e:\n",
    "    print(f\"Table check failed for {table} with error {str(e)}\")\n",
    "    return False\n",
    "\n",
    "failed_table_writes = []\n",
    "\n",
    "for t in table_metadata:\n",
    "  table_list = t.table_names_original\n",
    "  schema_name = t.db_id\n",
    "\n",
    "  try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
    "  except:\n",
    "    print(f\"Schema creation failed for {schema_name}\")\n",
    "    pass\n",
    "    \n",
    "  for t_name in table_list:\n",
    "    if table_exists(catalog, schema_name, t_name):\n",
    "      print(f\"Table already exists: {catalog}.{schema_name}.{t_name}. Skipping...\")\n",
    "      pass\n",
    "    else:\n",
    "      try: \n",
    "        con = sqlite3.connect(f\"{sql_lite_fqdn}{schema_name}/{schema_name}.sqlite\")\n",
    "        pdf = pd.read_sql_query(f\"SELECT * from `{t_name}`\", con)\n",
    "        con.close()\n",
    "\n",
    "        sdf = spark.createDataFrame(pdf)\n",
    "        sdf_final = clean_column_names(sdf)\n",
    "        sdf_final.write.saveAsTable(f\"{catalog}.{schema_name}.{t_name}\")\n",
    "      except Exception as e:\n",
    "        print(f\"Table creation failed for {t_name} with error: {str(e)}\")\n",
    "        failed_table_writes.append({\"schema\": schema_name, \"table\": t_name, \"error\": str(e)})\n",
    "        pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c930aef-c5fe-494a-9379-21f8d101fdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_table_writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6740fe2-376e-4e28-9df8-dac5ea5e8638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for ft in failed_table_writes:\n",
    "  volume_path = f\"{sql_lite_fqdn}{ft['schema']}/{ft['schema']}.sqlite\"\n",
    "  local_path = f\"/tmp/doan/bird/{ft['schema']}/{ft['schema']}.sqlite\"\n",
    "\n",
    "  if not os.path.exists(local_path):\n",
    "    dbutils.fs.cp(volume_path, f\"file:{local_path}\")\n",
    "  \n",
    "  con = sqlite3.connect(f\"file:{local_path}?mode=ro&immutable=1\")\n",
    "  pdf = pd.read_sql_query(f\"SELECT * from `{ft['table']}`\", con)\n",
    "  con.close()\n",
    "\n",
    "  sdf = spark.createDataFrame(pdf)\n",
    "  sdf_final = clean_column_names(sdf)\n",
    "  sdf_final.write.saveAsTable(f\"{catalog}.{ft['schema']}.{ft['table']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bc2c9f-d3d5-4379-b17b-a6cf299f4fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_setup_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
