{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2903500f-dba0-43b2-b754-2d3aa53a8f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./ingestion_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "databricks_config = config[\"databricks_config\"]\n",
    "volume_path = databricks_config[\"volume_path\"]\n",
    "\n",
    "catalog = config[\"databricks_config\"][\"catalog\"]\n",
    "metadata_schema = config[\"databricks_config\"][\"metadata_schema\"]\n",
    "schema_subset = config[\"databricks_config\"][\"db_subset\"].split(',')\n",
    "\n",
    "metadata_path = f\"{volume_path}dev_20240627/dev_tables.json\"\n",
    "sql_lite_fqdn = f\"{volume_path}dev_20240627/dev_databases/dev_databases/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d43e112-8505-4572-b9d6-e72eeee04ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if schema_subset:\n",
    "  dbs = schema_subset\n",
    "else:\n",
    "  db_records = spark.table(f\"{catalog}.{metadata_schema}.table_metadata\").select(\"db_id\").distinct().collect()\n",
    "  dbs = [db[\"db_id\"] for db in db_records]\n",
    "\n",
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9ace41-9ccc-4a16-8945-e3d4feea96ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for db in dbs:\n",
    "    for t in dbutils.fs.ls(f\"{sql_lite_fqdn}{db}/database_description_tsv\"):\n",
    "        table_name = t.name.replace(\".tsv\", \"\")\n",
    "        db_description = (\n",
    "            spark.read.option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .option(\"delimiter\", \"\\t\")\n",
    "            .csv(t.path)\n",
    "            .withColumn(\"table_name\", lit(table_name))\n",
    "        )\n",
    "        db_description.write.mode(\"append\").saveAsTable(f\"{catalog}.{db}.table_metadata\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_ingest_metadata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
