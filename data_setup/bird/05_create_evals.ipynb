{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037371ad-54ce-4bd8-8ad5-f6f8cf8aa446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q sqlglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9533ea36-a1ed-4f9e-bfa4-5c5f0819b804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('./ingestion_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "databricks_config = config[\"databricks_config\"]\n",
    "volume_path = databricks_config[\"volume_path\"]\n",
    "\n",
    "catalog = config[\"databricks_config\"][\"catalog\"]\n",
    "eval_schema = config[\"databricks_config\"][\"eval_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efba9dd0-7f7f-4a3c-bf1c-90ea173a5a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evals = spark.read.json(f\"{volume_path}/dev_20240627/dev.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08af6ea4-0a6f-4a7f-8ec6-99a89c1b8e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f881843c-1b5f-4700-8adb-42a343baa9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{eval_schema}\")\n",
    "  evals.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{eval_schema}.evals\")\n",
    "except Exception as e:\n",
    "  print(f\"Failed to create table with exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dd4ad7-c0ba-45dc-95f2-7d8d32920f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "import sqlglot                                        # loaded once per executor\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def _transpile_single(q: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Helper that turns one statement into Databricks SQL.\n",
    "    Returns None for blank / NULL inputs so filters still work.\n",
    "    \"\"\"\n",
    "    if q is None or not q.strip():\n",
    "        return None\n",
    "    return sqlglot.transpile(q, write=\"databricks\", pretty=False)[0]\n",
    "\n",
    "@pandas_udf(StringType())                                    # ↙ type‑hinted iterator\n",
    "def to_dbsql(batches: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Iterator‑of‑Series → Databricks SQL (fast Arrow batches).\"\"\"\n",
    "    for pdf in batches:                                      # pdf == Pandas Series\n",
    "        yield pdf.apply(_transpile_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd63c12-8ab5-4662-a9f0-e84357377e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#  UDF: fast + fault‑tolerant SQL → Databricks SQL transpiler\n",
    "# ---------------------------------------------------------------------------\n",
    "from typing import Iterator\n",
    "import re\n",
    "import pandas as pd\n",
    "import sqlglot\n",
    "from sqlglot.errors import ParseError\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "_BACKTICK_ID = re.compile(r\"`([^`]+)`\")          #  back‑ticked identifier\n",
    "\n",
    "def _ansi_quote(match: re.Match) -> str:\n",
    "    \"\"\"`foo bar`  ->  \"foo bar\"  (ANSI identifier).\"\"\"\n",
    "    return '\"' + match.group(1) + '\"'\n",
    "\n",
    "def _dbl_to_backtick(sql: str) -> str:\n",
    "    \"\"\"flip all ANSI quotes back to Databricks style.\"\"\"\n",
    "    # sqlglot always outputs identifiers as double‑quotes when the target\n",
    "    # dialect supports them, so a simple replace is safe here.\n",
    "    return sql.replace('\"', '`')\n",
    "\n",
    "def _safe_transpile(q: str | None,\n",
    "                    source_dialect: str = \"spark\") -> str | None:\n",
    "    if q is None or not q.strip():\n",
    "        return None\n",
    "\n",
    "    # 1️⃣  re‑quote problem identifiers\n",
    "    prepped = _BACKTICK_ID.sub(_ansi_quote, q)\n",
    "\n",
    "    try:\n",
    "        # 2️⃣  let sqlglot try again\n",
    "        t = sqlglot.transpile(\n",
    "            prepped,\n",
    "            read=source_dialect,   # tell sqlglot what it’s reading\n",
    "            write=\"databricks\",\n",
    "            pretty=False,\n",
    "        )[0]\n",
    "        return _dbl_to_backtick(t)           # restore back‑ticks\n",
    "\n",
    "    except ParseError:\n",
    "        # 3️⃣  give up gracefully – keep original SQL\n",
    "        return q\n",
    "\n",
    "# --- Vectorised iterator‑style Pandas UDF -----------------------------------\n",
    "@pandas_udf(StringType())                       # Spark infers SCALAR_ITER\n",
    "def to_dbsql(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for series in batch_iter:                   # each Arrow batch as a Series\n",
    "        yield series.apply(_safe_transpile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23419c2-0652-435a-bb5b-7ceda24069d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evals_gold = evals.withColumn(\"gold_SQL\", to_dbsql(col(\"SQL\")))\n",
    "display(evals_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93914bec-1cfb-4417-bd70-80763df217e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{eval_schema}\")\n",
    "  evals_gold.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{eval_schema}.evals_gold\")\n",
    "except Exception as e:\n",
    "  print(f\"Failed to create table with exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c3ef988-9f3b-4421-a172-11b9401b5ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05_create_evals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
