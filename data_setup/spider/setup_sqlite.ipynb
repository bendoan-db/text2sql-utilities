{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b40a69-74d1-4f32-b74f-65ff6cf15244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spider2_volumes_path = \"/Volumes/doan_spider/default/spider2-sqlite\"\n",
    "dbfs_target_path =\"dbfs:/doan/spider2\"\n",
    "\n",
    "files = [{'path': f.path, 'name': f.name} for f in dbutils.fs.ls(spider2_volumes_path)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1dedce-2bb0-4767-b10f-1a93fab02b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "  dbutils.fs.cp(f['path'], dbfs_target_path + \"/\" + f['name'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3e6282-8cf5-416e-bc23-5948f032e30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbfs_spider_files = dbutils.fs.ls(dbfs_target_path)\n",
    "\n",
    "for dbfs_file in dbfs_spider_files[:2]:\n",
    "  jdbc_url_prefix = \"jdbc:sqlite:\"\n",
    "  file_path = dbfs_file.path\n",
    "  schema_name = dbfs_file.name.split('.sqlite')[0]\n",
    "  print(f\"{jdbc_url_prefix}/{file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c07f640-3787-4950-9a85-5aa9da54ee8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for dbfs_file in dbfs_spider_files[:2]:\n",
    "  jdbc_url_prefix = \"jdbc:sqlite:\"\n",
    "  file_path = dbfs_file.path.replace('dbfs:/', 'dbfs/')\n",
    "  schema_name = dbfs_file.name.split('.sqlite')[0]\n",
    "  print(f\"{jdbc_url_prefix}/{file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f622679c-fed8-45d9-bc49-f06cda8e61df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(catalog, schema, table):\n",
    "  try:\n",
    "    query = f\"SHOW TABLES IN {catalog}.{schema} LIKE '{table}'\"\n",
    "    result = spark.sql(query).collect()\n",
    "    return len(result) > 0\n",
    "  except Exception as e:\n",
    "    print(f\"table check failed for {table}\")\n",
    "    return False\n",
    "  \n",
    "catalog = \"doan_spider\"\n",
    "dbfs_spider_files = dbutils.fs.ls(dbfs_target_path)\n",
    "\n",
    "failed_tables = []\n",
    "failed_count = 0\n",
    "\n",
    "for dbfs_file in dbfs_spider_files:\n",
    "  \n",
    "  #instantiate files\n",
    "  jdbc_url_prefix = \"jdbc:sqlite:\"\n",
    "  file_path = dbfs_file.path.replace('dbfs:/', 'dbfs/')\n",
    "  schema_name = dbfs_file.name.split('.sqlite')[0].replace('-', '_')\n",
    "\n",
    "  try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
    "\n",
    "    sqlite_tables = [\n",
    "      t.tbl_name\n",
    "      for t in (\n",
    "          spark.read.format(\"jdbc\")\n",
    "          .option(\"url\", f\"{jdbc_url_prefix}/{file_path}\")\n",
    "          .option(\"dbtable\", \"sqlite_master\")  # or any valid SQL sub-query in parentheses\n",
    "          .option(\"driver\", \"org.sqlite.JDBC\")\n",
    "          .load().select(\"tbl_name\").dropDuplicates()\n",
    "      ).collect()\n",
    "    ]\n",
    "  except Exception as e:\n",
    "    print(f\"Failed schema creation: {schema_name}\")\n",
    "    failed_tables.append({\n",
    "        \"schema\": schema_name,\n",
    "        \"table\": \"none - failed schema creation\",\n",
    "        \"error\":str(e)\n",
    "      })\n",
    "\n",
    "  for table in sqlite_tables:\n",
    "    if table_exists(catalog, schema_name, table):\n",
    "      print(f\"Table: {catalog}.{schema_name}.{table} exists. Skipping. \\n\")\n",
    "      pass\n",
    "    else:\n",
    "      try:\n",
    "        df = (\n",
    "            spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", f\"{jdbc_url_prefix}/{file_path}\")\n",
    "            .option(\"dbtable\", table)  # or any valid SQL sub-query in parentheses\n",
    "            .option(\"driver\", \"org.sqlite.JDBC\")\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema_name}.{table}\")\n",
    "      except Exception as e:\n",
    "        failed_tables.append({\n",
    "          \"schema\": schema_name,\n",
    "          \"table\": table,\n",
    "          \"error\":str(e)\n",
    "        })\n",
    "        failed_count = failed_count + 1\n",
    "        print(f\"Failed to write table: {catalog}.{schema_name}.{table}\")\n",
    "        print(f\"Failed table writes: {str(failed_count)}\")\n",
    "        print(f\"\\n\\n\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbfc43d-73c5-435e-97b9-f13bde99b255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "failed_questions = spark.createDataFrame(failed_tables).withColumn(\"process_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7edb289-8312-4492-a417-24a74deb4c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_questions.write.saveAsTable(\"doan_spider.default.failed_ingestion_tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc0c181-f79b-4bc1-955c-ad4e00ede751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"doan_spider.default.failed_ingestion_tables\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ba9e1f-06e4-465f-89f4-ec61b40ddc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "setup_sqlite",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
